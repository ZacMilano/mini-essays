The only "emoji picker" that Apple provides access to in their iOS software development kit is the emoji keyboard. I'm working on **[a certain mobile app](https://flippyco.in)** whose UX heavily relies on the action of picking one emoji at a time. I think I can do better: I want to make my own emoji picker with simple and intuitive UX.

## Here's the process I'm thinking:

First, find an up-to-date emoji data source, ideally **[rich in metadata](https://github.com/iamcal/emoji-data)**. Then, build a flat list of words associated with each emoji. I can toss these words into the **[super-cheap](https://platform.openai.com/docs/guides/embeddings/embedding-models)** embedding model from OpenAI, which gives a vector that **[embeds the concept of a piece of text](https://platform.openai.com/docs/guides/embeddings/what-are-embeddings)** into numbers, in a high-dimensional vector space.

Before build-time (of the app), I will collate and embed all text that I can find related to each specific emoji, and store that embedding vector locally, with easy lookup for performance & my own sanity. At runtime, fetch an embedding for user's emoji search query from the API. With this result, compute a **[matrix multiplication](https://cookbook.openai.com/examples/semantic_text_search_using_embeddings)** with the vector from this query, and the embedding matrix build by stacking the full emoji list's embedding vectors perpendicular to the query embedding vector. Then, select the top 10-20, rank them, and show them as the top results.

Below that, highlight the normal list of emojis based on the similarity to the search query. The top match has full highlight, bottom of the list has no highlight. This could be located below the top search results, and just convey the level of "probably-what-you-want"-ness via the highlight. Regarding execution, this highlight could come in the form of altered opacity, background color, brightness, (de)saturation, etc, I don't really care at the moment.

I could later try to cluster emoji in 2D space using the same model, and then spread them out using **[Lloyd's algorithm](https://en.wikipedia.org/wiki/Lloyd's_algorithm)**. (I learned about that in **[this video](https://www.youtube.com/watch?v=Bxdt6T_1qgc)**!) I'd love to further embed the points live on a closed (spherical or toroidal) space, somehow, so that users can infinitely scroll in all directions and (more or less) smoothly move between related emoji.