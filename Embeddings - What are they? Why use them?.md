To embed something is to reduce something about it (physical size, digital size) while retaining enough underlying information to be able to reproduce the original, pre-embedding object.

Commonly in machine learning models, an embedding model $M(c) \rightarrow R^N$ encodes an input concept $c$ into a collection of $N$ numbers, which we call a Vector in $N$-dimensional space. The model $M$ compresses the underlying information contained in the concept $c$, squashing it down into a list of numbers, or a point in high-dimensional space (depending on how you think about vectors).

I will return to this on a later date.

I am toast.

Toast again.

I might not continue with this one; I'll at least need to do some more research, because I don't fully understand it!
